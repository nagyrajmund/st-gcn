{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"c91df4db-6aab-4eb5-adc1-678027feb029"},"source":"# Insert code here.\nimport sys\nmodule_path = os.path.abspath(os.path.join('..'))\nprint(module_path)\nif module_path not in sys.path:\n    sys.path.append(module_path)","execution_count":1,"outputs":[{"name":"stdout","text":"/home/jovyan/work/src\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a248f6f9-331f-4312-9bac-7ebe4d07f4b7"},"source":"module_path = os.path.abspath(os.path.join('..'))\nfor dir in os.listdir('../'):\n    path = module_path + '/' + dir\n    sys.path.append(path)","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"22c14799-5872-4138-90cf-23b94d53365a"},"source":"!ls /datasets/kth_action | wc -l","execution_count":3,"outputs":[{"name":"stdout","text":"600\r\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"855d79d4-2b31-4a92-ab82-70fb37763eaa"},"source":"# TODO: this should be added to every runnable script to make imports work\n# Eventually we can use python\nimport sys\nsys.path.append('../')\n\nfrom data.datasets import KTHDataset\nfrom network import stgcn\nfrom torch import optim, nn, utils, autograd\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom data.util import loopy_pad_collate_fn\n\n\ndef train_network(config):\n    \"\"\"\n    Initialise dataset and network.\n\n    Parameters:\n        config:  map containing relevant parameters\n    \"\"\"\n\n    # TODO tidy config\n    dataset = KTHDataset(config['metadata_file'], config['dataset_dir'], use_confidence_scores=False)\n    dataloader = DataLoader(dataset, batch_size=config['batch_size'], sampler=config['sampler'],\n                            collate_fn=loopy_pad_collate_fn)\n    model = stgcn.STGCN(config['C_in'], config['gamma'], config['nr_classes'], edge_importance=config['edge_importance_weighting'])\n    model.to(config['device'])\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    # for name, param in model.named_parameters():\n    #     if param.device != config['device']:\n    #         print('param {}, not on GPU'.format(name))\n    #     # print(param.device)\n    #     input()\n\n    autograd.set_detect_anomaly(True) # enable anomaly detection TODO @amrita remove for debugging purposes only\n\n    # train\n    for epoch in range(config['n_epochs']):\n\n        for batch_idx, (data, label) in enumerate(dataloader):\n            if batch_idx == len(dataset) // 2:\n                break\n\n            data, label = data.to(config['device']), label.to(config['device'])  # Move to GPU\n\n            optimizer.zero_grad() # pytorch accumulates gradients on every call to loss.backward() so need to 0 gradients to get correct parameter update\n            output = model.forward(data.float())\n\n            loss = criterion(output, label)\n            # TODO @amrita add loss_train, loss_val\n            # TODO @amrita need to double check this is okay, needed for when edge importance weighting is used\n            if batch_idx == 0:\n                loss.backward(retain_graph = True)\n            else:\n                loss.backward()  # Backward pass\n            optimizer.step()  # Update the weights\n\n            \n\n            if batch_idx == config['batch_size'] - 1:\n                break\n        if epoch % 10 == 0:\n                print('Epoch: ', epoch + 1, '\\t loss: ', loss)\n\n\n    # get prediction\n    # TODO replace with test set\n    # pred = torch.argmax(model(test_data), dim=1)\n\n\n\n\n","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"29c04bc0-b48b-4145-bcb0-52c0978c8ad1"},"source":"import torch\n\ndataset_dir = '/datasets/kth_action' # for deepnote TODO change\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint('Using device:', device)\n\nconfig = {\n    'dataset_dir': Path(dataset_dir),\n    'metadata_file': Path(dataset_dir) / 'metadata.csv',\n    'batch_size': 16,\n    'n_epochs': 20,\n    'nr_classes': 6,\n    'device': device,\n    'sampler': None,  # Sampler can handle randomization etc.\n    'C_in': 2,  # number of input channels\n    'gamma': 9,  # temporal convolution kernel size\n    'edge_importance_weighting': True  # whether to use edge importance weighting\n}\n\ntrain_network(config)","execution_count":5,"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch:  1 \t loss:  tensor(5.2036, device='cuda:0', grad_fn=<NllLossBackward>)\nEpoch:  11 \t loss:  tensor(1.7669, device='cuda:0', grad_fn=<NllLossBackward>)\n","output_type":"stream","truncated":false}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8f98c8a2-fb6d-47e4-a74f-577731fdd47e"},"source":"","execution_count":6,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"1eb53546-5d08-4e52-87d0-d9383f5bfa70","deepnote_execution_queue":[]}}